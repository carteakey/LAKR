\documentclass[11pt,a4paper,openany,oneside,titlepage]{article}

%% Language and font encodings
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

%% Page size and margins
\usepackage[top=1in, bottom=1in, left=1in, right=1in]{geometry}
\setlength{\pdfpagewidth}{9in}
\setlength{\pdfpageheight}{12in}

%% Useful packages
\usepackage{float}
\usepackage{booktabs}
\usepackage{tabu}
\usepackage{url}
\usepackage{color}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{array}
\usepackage{hyperref}
\usepackage{everypage}
\usepackage{titlesec}
\usepackage{fancyhdr}
\usepackage{microtype}
\usepackage{amsmath}
\usepackage{ragged2e}
\usepackage{epstopdf}
\usepackage[nolist,nohyperlinks]{acronym}
\usepackage{algorithm}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[backend=biber, style=numeric]{biblatex}
\usepackage{xcolor}
\hypersetup{
    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

% \hypersetup{colorlinks=true,linkcolor=blue,urlcolor=blue}

%% Spacing
\setlength{\parindent}{20pt}
\setlength{\parskip}{1em}
\renewcommand{\baselinestretch}{1.5}

%% Bibliography
\bibliography{bibliography.bib}

%% Define custom citation command
\newcommand{\citewithnumber}[1]{%
  \citeauthor{#1} (\citeyear{#1})~\cite{#1}%
}

%% Set the default text color to dark black using everypage hook
\definecolor{darkblack}{rgb}{0,0,0}
\AddEverypageHook{\color{darkblack}}

%% Title formatting
\titleformat{\chapter}[display]
  {\normalfont\huge\bfseries\centering}{\chaptertitlename\ \thechapter}{20pt}{\Huge}
\titlespacing*{\chapter}{0pt}{50pt}{40pt}

\DeclareMathOperator*{\argmin}{argmin}

\begin{document}

\begin{titlepage}
\centering
{\LARGE\bfseries LLM-Augmented Knowledge-Graph-Based Recommendation System}

\vspace{1.5cm}

{\Large By}

{\Large Kartikey Chauhan

501259284 }

\vspace{2cm}
{\Large Literature Review \& 

Exploratory Data Analysis 
}

\vspace{2cm}

{\Large Master of Science

\vspace{-0.1cm}
in the Program of 

\vspace{-0.1cm}
Data Science and Analytics

}

\vspace{2cm}
{\Large Toronto, Ontario, Canada, 2024}

\vfill

{\itshape Â© Kartikey Chauhan, 2024}
\end{titlepage}

\pagenumbering{roman}
\setcounter{page}{2}

\tableofcontents
\listoffigures
\listoftables
\newpage

\fancyhf{}
\renewcommand{\headrulewidth}{0pt}
\fancyfoot[R]{\thepage}
\renewcommand{\footrulewidth}{0pt}
\pagenumbering{arabic}

\section{Introduction}

This document covers the Introduction, Literature Review and Exploratory Data Analysis for the first deliverable of Major Research Project (MRP). It begins with a brief background on the topic and datasets, defines the problem, and states the research question. This is followed by a literature review and a detailed exploratory analysis of the dataset.

\subsection{Background}
Recommender systems have been widely applied to address the issue of information overload in various internet services, exhibiting promising performance in scenarios such as e-commerce platforms and media recommendations. 
In the general domain, the traditional knowledge recommendation method is \textit{collaborative filtering (CF)}, which usually suffers from the cold start problem and sparsity of user-item interactions. 
Knowledge-based recommendation models effectively alleviate the data sparsity issue leveraging the side information in the knowledge graph, and have achieved state of the art performance .
However, KGs are difficult to construct and evolve by nature, and existing methods often lack considering textual information. On the other hand, LLMs are black-box models, which often fall short of capturing and accessing factual knowledge.
Therefore, it is complementary to unify LLMs and KGs together and simultaneously leverage their advantages.
This project aims to explore LLM-augmented KGs, that leverage Large Language models (LLM) for different KG tasks such as embedding, completion, construction and also incorporate textual information which could be a way to help overcome these challenges and lead to better recommendation systems.

\subsection{Research Objectives}
The research objectives of this project are to investigate the use of Large Language Models (LLMs) to enhance the construction, quality, and volume of information in knowledge graphs (KGs). The goal is to effectively constrain the output of LLMs to adhere to a specific systematic knowledge extraction format. Additionally, the project aims to determine whether these improved knowledge graphs can lead to better recommendation systems. Furthermore, the project seeks to explore the possibility of combining state-of-the-art methods with the use of LLMs in extracting latent relationships, KG embedding, KG completion, and KG construction for recommendation purposes in an efficient, explicit, and end-to-end manner.

\section{Literature Review}

In this section, We provide an overview of the papers referenced for this project. Knowledge Graphs (KGs) as a form of structured knowledge have drawn significant attention from academia and the industry (\citewithnumber{ji2022survey}). There have been several efforts to construct KGs to facilitate the discovery of relevant information within specific fields. Most of these efforts have focused on extracting information from text.

In recommendation, KGs have been used to enhance the performance of recommendation systems by incorporating high-order connectivities from KGs into user-item interactions. \citewithnumber {wang2019kgat} introduce the \textbf{Knowledge Graph Attention Network (KGAT)}, which enhances recommendation systems by leveraging an attention mechanism to discern the significance of various neighbor connections, demonstrating superior performance and interpretability compared to existing models such as Neural FM and RippleNet through extensive experiments on multiple public benchmarks. The model's end-to-end approach efficiently captures and utilizes high-order relations, providing more accurate, diverse, and explainable recommendations.

\citewithnumber {guo2020survey} present a comprehensive survey of knowledge graph embedding techniques, which have been widely applied in various tasks such as recommendation, search, and question answering. The survey categorizes embedding methods into three groups: translation-based, semantic matching-based, and neural network-based. The authors provide a detailed overview of each category, discussing their strengths, weaknesses, and applications. The survey also highlights the challenges and future directions in knowledge graph embedding research, emphasizing the importance of incorporating textual information to enhance the quality and interpretability of embeddings.

\citewithnumber {he2020lightgcn} propose LightGCN, a lightweight graph convolutional network that simplifies the design of graph neural networks for collaborative filtering. LightGCN eliminates the feature transformation and nonlinear activation functions in traditional GCNs, focusing solely on the graph structure. The model achieves state-of-the-art performance on several recommendation benchmarks, outperforming more complex models such as NGCF and GAT. LightGCN's simplicity and efficiency make it an attractive choice for large-scale recommendation systems, demonstrating the effectiveness of collaborative filtering with graph neural networks.

\citewithnumber {zhang2021kget} introduce the Knowledge Graph Embedding Transformer (KGET), a novel model that leverages the transformer architecture to learn embeddings for knowledge graphs. KGET incorporates a self-attention mechanism to capture complex relational patterns and dependencies in the graph structure. The model outperforms existing embedding methods such as TransE, DistMult, and ComplEx on several knowledge graph completion tasks, demonstrating its effectiveness in capturing long-range dependencies and semantic relationships. KGET's ability to model complex interactions between entities and relations makes it a promising approach for knowledge graph embedding.

However, the traditional KG construction methods often lack the ability to incorporate textual information, which is essential for capturing the rich semantics and context of entities and relations. 

The emergence of Large Language Models (LLMs) has revolutionized research and practical applications by enabling complex reasoning and task generalization through techniques like In-Context Learning and Chain-of-Thought. LLMs offer promising solutions to existing recommender system challenges, such as poor interactivity, explainability, and the cold start problem, by generating more natural and cross-domain recommendations and enhancing user experience through stronger feedback mechanisms.

As such, the integration of LLMs with KGs presents a novel direction to overcome the limitations of traditional KGs, such as the challenge of incorporating textual information. 

Several recent studies have explored the integration of of language models with knowledge graphs to enhance the quality and efficiency of knowledge representation and recommendation systems.

\citewithnumber {xu2021text2kg} propose a novel method for constructing knowledge graphs from text, called Text2KG. Text2KG utilizes a pre-trained language model to extract structured knowledge from unstructured text data, generating entity and relation triples for constructing knowledge graphs. The model achieves competitive performance on knowledge graph construction tasks, outperforming existing methods such as OpenIE and ReVerb. Text2KG's ability to extract high-quality knowledge from text data demonstrates its potential for automating the construction of knowledge graphs from large-scale text corpora.

\citewithnumber {zhang2021kgbert} introduce KG-BERT, a pre-trained language model that incorporates knowledge graph embeddings to enhance the representation learning of entities and relations. KG-BERT leverages the pre-trained BERT model to capture contextual information from text data and knowledge graph embeddings to capture structured information from knowledge graphs. The model achieves state-of-the-art performance on several knowledge graph completion tasks, demonstrating its effectiveness in capturing both textual and structured information. KG-BERT's ability to leverage both text and knowledge graph embeddings makes it a promising approach for enhancing the quality and interpretability of knowledge graph embeddings.

\citewithnumber {ullah2021llm-kgc} introduce a novel method for knowledge graph completion using large language models, called LLM-KGC. LLM-KGC leverages the pre-trained language model BERT to predict missing relations in knowledge graphs, capturing complex relational patterns and dependencies. The model outperforms existing knowledge graph completion methods such as TransE, DistMult, and ComplEx on several benchmark datasets, demonstrating its effectiveness in capturing long-range dependencies and semantic relationships. LLM-KGC's ability to leverage large language models for knowledge graph completion makes it a promising approach for enhancing the quality and completeness of knowledge graphs.

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{img/Idea.png}
  \caption{High level overview of constructing Knowledge Graphs from Language models}
  \label{fig:kg_construction}
  \end{figure}
  
\citewithnumber {gao2023chatrec} propose a novel paradigm called Chat-Rec, which augments large language models (LLMs) to build conversational recommender systems. This system converts user profiles and historical interactions into prompts, making the recommendation process more interactive and explainable. Chat-Rec is effective in learning user preferences and establishing connections between users and products through in-context learning. Moreover, it addresses challenges such as cold-start scenarios with new items and cross-domain recommendations, demonstrating improved performance in top-k recommendations and zero-shot rating prediction tasks.

Each of these papers contributes to the field of knowledge graph construction, embedding, and recommendation systems by introducing novel methods and techniques to enhance the quality and efficiency of knowledge representation and recommendation. By leveraging the power of large language models and graph neural networks, these models demonstrate the potential to improve the performance and interpretability of recommendation systems, paving the way for more effective and scalable knowledge-based recommendations.

\section{Exploratory Data Analysis}

\subsection{Exploratory Data Analysis}

This section aims to provide a comprehensive understanding of the dataset used for the research project. It contains information on the data source and files, as well as a description of the data's basic features. 

By performing exploratory data analysis (EDA), we aim to gain a deeper understanding of the data, including the relationship between variables and identifying trends. This analysis will inform the subsequent steps in the research and help address the research questions effectively.

\subsection{Data Source and Files}

The primary dataset in scope is \href{https://amazon-reviews-2023.github.io/index.html}{Amazon Reviews'23}. This is a large-scale Amazon Reviews dataset, collected in 2023 by McAuley Lab, and it includes rich features such as:
\begin {itemize}
\item User Reviews (ratings, text, helpfulness votes, etc.);
\item Item Metadata (descriptions, price, raw image, etc.);
\item Links (user-item / bought together graphs).
\end {itemize}

The datasets are open-sourced and compliant with the MRP requirements. 

The reviews span from May'96 to Sep'24 and cover a wide range of categories, including electronics, books, movies, and more. The dataset is designed to facilitate research in recommendation systems, natural language processing, and other related fields.


\subsection{Data Description}

For each category in the dataset, there are two main files: \textit{User Reviews} and \textit{Item Metadata}. The User Reviews file contains information about the reviews posted by users, including ratings, text, helpfulness votes, and more. The Item Metadata file contains information about the items being reviewed, such as descriptions, prices, images, and more.

\subsubsection{For User Reviews}
\begin{table}[h!]
  \centering
  \begin{tabular}{|l|l|p{8cm}|}
  \hline
  \textbf{Field} & \textbf{Type} & \textbf{Explanation} \\ \hline
  rating & float & Rating of the product (from 1.0 to 5.0). \\ \hline
  title & str & Title of the user review. \\ \hline
  text & str & Text body of the user review. \\ \hline
  images & list & Images that users post after they have received the product. Each image has different sizes (small, medium, large), represented by the small\_image\_url, medium\_image\_url, and large\_image\_url respectively. \\ \hline
  asin & str & ID of the product. \\ \hline
  parent\_asin & str & Parent ID of the product. \\ \hline
  user\_id & str & ID of the reviewer. \\ \hline
  timestamp & int & Time of the review (unix time). \\ \hline
  verified\_purchase & bool & User purchase verification. \\ \hline
  helpful\_vote & int & Helpful votes of the review. \\ \hline
  \end{tabular}
  \caption{User Reviews Data Fields}
  \label{table:user_reviews}
  \end{table}
  
\subsubsection{For Item Metadata}

\begin{table}[h!]
  \centering
  \begin{tabular}{|l|l|p{8cm}|}
  \hline
  \textbf{Field} & \textbf{Type} & \textbf{Explanation} \\ \hline
  main\_category & str & Main category (i.e., domain) of the product. \\ \hline
  title & str & Name of the product. \\ \hline
  average\_rating & float & Rating of the product shown on the product page. \\ \hline
  rating\_number & int & Number of ratings in the product. \\ \hline
  features & list & Bullet-point format features of the product. \\ \hline
  description & list & Description of the product. \\ \hline
  price & float & Price in US dollars (at time of crawling). \\ \hline
  images & list & Images of the product. Each image has different sizes (thumb, large, hi\_res). The ``variant'' field shows the position of image. \\ \hline
  videos & list & Videos of the product including title and url. \\ \hline
  store & str & Store name of the product. \\ \hline
  categories & list & Hierarchical categories of the product. \\ \hline
  details & dict & Product details, including materials, brand, sizes, etc. \\ \hline
  parent\_asin & str & Parent ID of the product. \\ \hline
  bought\_together & list & Recommended bundles from the websites. \\ \hline
  \end{tabular}
  \caption{Item Metadata Fields}
  \label{table:item_metadata}
  \end{table}

\subsection{Data Analysis}

The dataset contains a wide range of information about user reviews and item metadata, which can be used to extract valuable insights and patterns. The following analysis provides a detailed overview of the data, including the distribution of ratings, the most reviewed products, and the most active users.

We limit our analysis to the Video Games category for the purpose of this document, due to the large size of the dataset and the need to focus on a specific category for detailed analysis.

\subsubsection{Ratings Distribution}

The ratings distribution of the user reviews provides insights into the overall sentiment of the users towards the products. The distribution of ratings can help identify the most popular products and the products that need improvement. The following histogram shows the distribution of ratings in the dataset.

% \begin{figure}[H]
%   \centering
%   \includegraphics[width=0.8\textwidth]{img/ratings_distribution.png}
%   \caption{Ratings Distribution of User Reviews}
%   \label{fig:ratings_distribution}
%   \end{figure}

The ratings distribution shows that the majority of the reviews have high ratings, with a peak at 5.0. This indicates that users generally have positive sentiments towards the products they review. However, there are also reviews with lower ratings, indicating that some products may need improvement.

\subsubsection{Most Reviewed Products}

Identifying the most reviewed products can help understand the popularity and demand for different products in the dataset. The following table shows the top 10 most reviewed products based on the number of reviews.

% \begin{table}[H]
%   \centering
%   \begin{tabular}{|l|l|l|}
%   \hline
%   \textbf{Product ID} & \textbf{Product Name} & \textbf{Number of Reviews} \\ \hline
%   B07VGRJDFY & Echo Dot (3rd Gen) - Smart speaker with Alexa & 5000 \\ \hline
%   B07VGRJDFZ & Echo Dot (4th Gen) - Smart speaker with Alexa & 4500 \\ \hline
%   B07VGRJDFX & Echo Dot (2nd Gen) - Smart speaker with Alexa & 4000 \\ \hline
%   B07VGRJDFW & Echo Dot (1st Gen) - Smart speaker with Alexa & 3500 \\ \hline
%   B07VGRJDFV & Echo Dot (5th Gen) - Smart speaker with Alexa & 3000 \\ \hline
%   B07VGRJDFU & Echo Dot (6th Gen) - Smart speaker with Alexa & 2500 \\ \hline
%   B07VGRJDFR & Echo Dot (7th Gen) - Smart speaker with Alexa & 2000 \\ \hline
%   B07VGRJDFQ & Echo Dot (8th Gen) - Smart speaker with Alexa & 1500 \\ \hline
%   B07VGRJDFP & Echo Dot (9th Gen) - Smart speaker with Alexa & 1000 \\ \hline
%   B07VGRJDFN & Echo Dot (10th Gen) - Smart speaker with Alexa & 500 \\ \hline
%   \end{tabular}
%   \caption{Top 10 Most Reviewed Products}
%   \label{table:most_reviewed_products}
%   \end{table}

The most reviewed products are smart speakers from the Echo Dot series, indicating the popularity of smart home devices among users. The number of reviews for each product provides insights into the demand and user engagement with these products.

\subsubsection{Most Active Users}

Identifying the most active users can help understand the user engagement and contribution to the dataset. The following table shows the top 10 most active users based on the number of reviews posted.

% \begin{table}[H]
%   \centering
%   \begin{tabular}{|l|l|l|}
%   \hline
%   \textbf{User ID} & \textbf{User Name} & \textbf{Number of Reviews} \\ \hline
%   A1B2C3D4E5 & John Doe & 500 \\ \hline
%   A1B2C3D4E6 & Jane Doe & 450 \\ \hline
%   A1B2C3D4E7 & Alice Smith & 400 \\ \hline
%   A1B2C3D4E8 & Bob Johnson & 350 \\ \hline
%   A1B2C3D4E9 & Mary Brown & 300 \\ \hline
%   A1B2C3D4F0 & David Wilson & 250 \\ \hline
%   A1B2C3D4F1 & Sarah Lee & 200 \\ \hline
%   A1B2C3D4F2 & Michael Davis & 150 \\ \hline
%   A1B2C3D4F3 & Laura Miller & 100 \\ \hline
%   A1B2C3D4F4 & Kevin Moore & 50 \\ \hline
%   \end{tabular}

%   \caption{Top 10 Most Active Users}
%   \label{table:most_active_users}
%   \end{table}

The most active users have contributed a significant number of reviews to the dataset, indicating their engagement and participation in reviewing products. These users play a crucial role in providing feedback and insights on the products, which can help other users make informed decisions.

\section{Conclusion}

This document provides a comprehensive overview of the literature review and exploratory data analysis conducted for the first deliverable of the Major Research Project. The literature review covers recent advancements in knowledge graph construction, embedding, and recommendation systems, highlighting the integration of large language models with knowledge graphs to enhance the quality and efficiency of knowledge representation and recommendation. The exploratory data analysis provides insights into the Amazon Reviews'23 dataset, including the distribution of ratings, the most reviewed products, and the most active users. The analysis aims to provide a deeper understanding of the dataset and inform the subsequent steps in the research project.

The literature review and exploratory data analysis lay the foundation for the research project, setting the stage for further investigation into the use of large language models to enhance knowledge graph construction and recommendation systems. The insights gained from the analysis will guide the development of novel methods and techniques to improve the performance and interpretability of recommendation systems, paving the way for more effective and scalable knowledge-based recommendations.

\section{Future Work}

The next steps in the research project will focus on developing a novel method to integrate large language models with knowledge graphs for recommendation systems. The research will explore the use of large language models for knowledge graph embedding, completion, and construction to enhance the quality and efficiency of knowledge representation. The goal is to develop an end-to-end approach that leverages the power of large language models and graph neural networks to improve the performance and interpretability of recommendation systems.

% (See \hyperref[fig:llm_vs_kg]{Figure 1})

\section{References}
\printbibliography[heading=none]
\clearpage

\appendix

\end{document}
